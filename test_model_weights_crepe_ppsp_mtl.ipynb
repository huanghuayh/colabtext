{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# %matplotlib widget\n",
    "import matplotlib\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "matplotlib.use('QT5Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_dict(loaded_dict, row_indices=[0, 4], col_size=512, output_format=\"channels_first\"):\n",
    "    processed_data = {}\n",
    "\n",
    "    for key, (test_x, test_y, fund_freq_lst, distances_lst, file_names_lst, orig_signal_lst) in loaded_dict.items():\n",
    "        test_x = np.array(test_x)\n",
    "        test_y = np.array(test_y)\n",
    "        distances_lst = np.array(distances_lst)\n",
    "        fund_freq_lst = np.array(fund_freq_lst)\n",
    "        orig_signal_lst = np.array(orig_signal_lst)\n",
    "\n",
    "        # Select rows and columns size (preserve original order - no sorting)\n",
    "        X = test_x[:, row_indices, :col_size]\n",
    "        y = test_y\n",
    "\n",
    "        # Reshape based on desired output format\n",
    "        if output_format == \"channels_first\":\n",
    "            X = X[:, :, np.newaxis, :]  # [N, num_channels, 1, col_size]\n",
    "        elif output_format == \"channels_last\":\n",
    "\n",
    "            X = X[:, np.newaxis, :, :]  # [N, 1, num_channels, col_size]\n",
    "        else:\n",
    "            raise ValueError(\"output_format must be 'channels_first' or 'channels_last'\")\n",
    "\n",
    "        # Return all data in original order (no train/val split)\n",
    "        processed_data[key] = (X, y, fund_freq_lst, distances_lst, file_names_lst, orig_signal_lst)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GradCAM class\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        ### Register hooks\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def __call__(self, x, target_category=None):\n",
    "        ### Forward pass\n",
    "        output = self.model(x)\n",
    "\n",
    "        if target_category is None:\n",
    "            ### Use the fundamental prediction [batch_size, 1, 1024] for Grad-CAM\n",
    "            target = output[0]\n",
    "        else:\n",
    "            target = output[target_category]\n",
    "\n",
    "        ### Zero gradients\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        ### For Grad-CAM, we need a scalar value (mean)\n",
    "        target_scalar = target.mean()\n",
    "\n",
    "        ### Backward pass for target\n",
    "        target_scalar.backward(retain_graph=True)\n",
    "\n",
    "        #### Get gradients and activations\n",
    "        # [batch_size, channels, length]\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "\n",
    "        ### Global average pooling of gradients across spatial dimension (length)\n",
    "        weights = torch.mean(gradients, dim=2)\n",
    "\n",
    "        # Weight the activations\n",
    "        batch_size, channels, length = activations.shape\n",
    "        cam = torch.zeros(batch_size, length, device=activations.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(channels):\n",
    "                cam[i] += weights[i, j] * activations[i, j, :]\n",
    "\n",
    "        # Apply ReLU\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalize\n",
    "        cam = cam - cam.min(dim=1, keepdim=True)[0]\n",
    "        cam = cam / (cam.max(dim=1, keepdim=True)[0] + 1e-8)\n",
    "\n",
    "        return cam.detach().cpu().numpy(), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED Grad-CAM for FPN_2_mtl model (from crepe.py)\n",
    "class GradCAM_FPN2MTL:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Register hooks\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def __call__(self, x, target_category=None):\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "\n",
    "        # For FPN_2_mtl, output is already the fundamental prediction\n",
    "        target = output\n",
    "\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Convert to scalar for backprop - use the actual prediction values\n",
    "        # We need to backpropagate through the actual predictions, not just the mean\n",
    "        target_scalar = target.mean()\n",
    "\n",
    "        # Backward pass for target\n",
    "        target_scalar.backward(retain_graph=True)\n",
    "\n",
    "        # Get gradients and activations\n",
    "        if self.gradients is None or self.activations is None:\n",
    "            raise RuntimeError(\"Gradients or activations not captured. Check target layer.\")\n",
    "\n",
    "        gradients = self.gradients  # [batch_size, channels, length]\n",
    "        activations = self.activations  # [batch_size, channels, length]\n",
    "\n",
    "        # Global average pooling of gradients across spatial dimension\n",
    "        weights = torch.mean(gradients, dim=2)  # [batch_size, channels]\n",
    "\n",
    "        # Weight the activations\n",
    "        batch_size, channels, length = activations.shape\n",
    "        cam = torch.zeros(batch_size, length, device=activations.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(channels):\n",
    "                cam[i] += weights[i, j] * activations[i, j, :]\n",
    "\n",
    "        # Apply ReLU\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalize\n",
    "        if cam.max() - cam.min() > 1e-8:\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        else:\n",
    "            cam = torch.zeros_like(cam)\n",
    "\n",
    "        return cam.detach().cpu().numpy(), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED Grad-CAM for PPSP model (from fpn_2.py)\n",
    "class GradCAM_PPSP:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Register hooks\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def __call__(self, x, target_category=None):\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "\n",
    "        # For PPSP, output is the final output3 [batch_size, 1, 1024]\n",
    "        target = output\n",
    "\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Convert to scalar for backprop\n",
    "        target_scalar = target.mean()\n",
    "\n",
    "        # Backward pass for target\n",
    "        target_scalar.backward(retain_graph=True)\n",
    "\n",
    "        # Get gradients and activations\n",
    "        if self.gradients is None or self.activations is None:\n",
    "            raise RuntimeError(\"Gradients or activations not captured. Check target layer.\")\n",
    "\n",
    "        gradients = self.gradients  # [batch_size, channels, length]\n",
    "        activations = self.activations  # [batch_size, channels, length]\n",
    "\n",
    "        # Global average pooling of gradients across spatial dimension\n",
    "        weights = torch.mean(gradients, dim=2)  # [batch_size, channels]\n",
    "\n",
    "        # Weight the activations\n",
    "        batch_size, channels, length = activations.shape\n",
    "        cam = torch.zeros(batch_size, length, device=activations.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(channels):\n",
    "                cam[i] += weights[i, j] * activations[i, j, :]\n",
    "\n",
    "        # Apply ReLU\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalize\n",
    "        if cam.max() - cam.min() > 1e-8:\n",
    "            cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        else:\n",
    "            cam = torch.zeros_like(cam)\n",
    "\n",
    "        return cam.detach().cpu().numpy(), output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing crepe and ppsp - CORRECTED VERSION\n",
    "\"\"\"\n",
    "import warnings, pickle\n",
    "import os, csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mode = \"block\"\n",
    "all_combs_lists = [[3]]\n",
    "import torch\n",
    "from models import fpn_2\n",
    "from models import crepe\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import fuzzy_logic as fl\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "with open(f\"../conv2d_data/conv2d_psd_scaled_sfnds_1up_block_test.pkl\", \"rb\") as f:\n",
    "    loaded_dict_test = pickle.load(f)\n",
    "\n",
    "for ind, comb_lst in enumerate(all_combs_lists):\n",
    "\n",
    "    dir_path = f\"./conv2d_data/pred_plots/{comb_lst}/\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Initialize FPN_2_mtl model\n",
    "    crepe_model = crepe.FPN_2_mtl(in_channels=len(comb_lst))\n",
    "\n",
    "    model_name = f\"../1up_weights/best_fpn2_1up_model_gaussian_[3].pth\"\n",
    "    crepe_model.load_state_dict(torch.load(f\"{model_name}\", map_location=torch.device('cpu')))\n",
    "    crepe_model.eval()\n",
    "\n",
    "    ### CORRECTED: Use the LAST convolutional layer before flattening\n",
    "    # Target the final conv_block10 in the encoder pathway\n",
    "    target_layer_crepe = crepe_model.conv_block10.c\n",
    "    grad_cam_crepe = GradCAM_FPN2MTL(crepe_model, target_layer_crepe)\n",
    "\n",
    "    processed_test = process_test_dict(\n",
    "        loaded_dict_test,\n",
    "        row_indices=comb_lst,\n",
    "        col_size=1024,\n",
    "        output_format=\"channels_last\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    for key, (X_test, y_test, fund_freq, distances, file_names, orig_sig) in processed_test.items():\n",
    "        print(f\"Key: {key}\")\n",
    "        cur_res_dict = {}\n",
    "        if key == key:\n",
    "            # cur_results = []\n",
    "            # csv_path = os.path.join(dir_path, f\"{key}_results.csv\")\n",
    "            # csv_file = open(csv_path, mode=\"w\", newline=\"\")\n",
    "            # writer = csv.writer(csv_file)\n",
    "            # writer.writerow([\"filename\", \"gtruth_fund_freq\", \"predicted_fund_freq\", \"predicted_fund_freq_lst\"])\n",
    "\n",
    "            for sample_ind in range(len(distances)):  # Test with 3 samples\n",
    "                cur_original_sig = orig_sig[sample_ind]\n",
    "                cur_fund_freq = fund_freq[sample_ind]\n",
    "                cur_fil_name = file_names[sample_ind]\n",
    "\n",
    "                cur_x = np.array(X_test[sample_ind])[:, :, :]\n",
    "                torch_x = torch.FloatTensor(cur_x).to('cpu')\n",
    "                cur_y = np.array(y_test[sample_ind])\n",
    "\n",
    "                ### Enable gradients for input\n",
    "                torch_x.requires_grad_()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    cur_prediction = crepe_model(torch_x)\n",
    "\n",
    "                fund_pred = torch.sigmoid(cur_prediction).squeeze(1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "                cur_fund_prediction_norm = (fund_pred - np.min(fund_pred)) / (\n",
    "                        np.max(fund_pred) - np.min(fund_pred) + 1e-12)\n",
    "                binary_cur_truth = cur_y\n",
    "\n",
    "                ### apply thresholding\n",
    "                bin_fund_pred = np.where(cur_fund_prediction_norm > 0.01, 1, 0)\n",
    "\n",
    "                ### Find windows\n",
    "                # fund_regions_lst, fund_central_freq_lst = fl.find_windows(bin_fund_pred)\n",
    "                #\n",
    "                # fine_pred_freq_lst = [fund_central_freq_lst[0]] if len(fund_central_freq_lst) > 0 else [0]\n",
    "                # print(f\"filename={cur_fil_name} gtruth={cur_fund_freq} predicted={fine_pred_freq_lst}\")\n",
    "                #\n",
    "                # fine_pred_freq = fund_central_freq_lst[0] if len(fund_central_freq_lst) > 0 else 0\n",
    "                #\n",
    "                # writer.writerow([cur_fil_name, cur_fund_freq, fine_pred_freq, fine_pred_freq_lst])\n",
    "                # cur_results.append(\n",
    "                #     [[bin_fund_pred], binary_cur_truth, cur_x, cur_fund_freq, fine_pred_freq,\n",
    "                #      cur_fil_name, fine_pred_freq_lst,])\n",
    "\n",
    "                ### Generate Grad-CAM for FPN_2_mtl\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    cam_crepe, _ = grad_cam_crepe(torch_x)\n",
    "\n",
    "                ### Convert CAM to same length as input\n",
    "                cam_crepe = cam_crepe.squeeze()\n",
    "\n",
    "                if cam_crepe.ndim > 1:\n",
    "                    cam_crepe = cam_crepe[0]\n",
    "\n",
    "                # The CAM might be shorter than 1024 due to downsampling, so we need to interpolate\n",
    "                cam_resized = np.interp(np.linspace(0, len(cam_crepe)-1, 1024),\n",
    "                                       np.arange(len(cam_crepe)), cam_crepe)\n",
    "\n",
    "        #         if distances[sample_ind] not in cur_res_dict:\n",
    "        #             cur_res_dict[distances[sample_ind]] = [[cur_fil_name, cur_fund_freq,X_test[sample_ind],y_test[sample_ind],fund_pred,cur_fund_prediction_norm,binary_cur_truth,cam_crepe,cam_resized]]\n",
    "        #         else:\n",
    "        #             cur_res_dict[distances[sample_ind]].append([cur_fil_name, cur_fund_freq,X_test[sample_ind],y_test[sample_ind],fund_pred,cur_fund_prediction_norm,binary_cur_truth,cam_crepe,cam_resized])\n",
    "        #\n",
    "        # pkl_path = os.path.join(dir_path, f\"{key}_crepe_cam\")\n",
    "        # pickle.dump([cur_res_dict], open(f\"{pkl_path}\", \"wb\"))\n",
    "\n",
    "                # Plot with Grad-CAM\n",
    "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "                # Plot 1: Input signals with predictions\n",
    "                for ind_x in range(cur_x.squeeze(0).shape[0]):\n",
    "                    ax1.plot(cur_x.squeeze(0)[ind_x], linewidth=1.2, alpha=0.7, label=f'Input {ind_x}' if ind_x == 0 else \"\")\n",
    "\n",
    "                ax1.plot(binary_cur_truth, linewidth=1.5, label=\"Ground Truth\", alpha=0.9, color='black')\n",
    "                ax1.plot(cur_fund_prediction_norm, linewidth=1.5, label=\"f0 Prediction\", alpha=0.8, color='blue')\n",
    "                ax1.plot(bin_fund_pred, linewidth=1.2, label=\"Binary f0\", alpha=0.6, color='green')\n",
    "                ax1.legend(loc='upper right')\n",
    "                ax1.set_title(f\"FPN_2_mtl - {cur_fund_freq}Hz - {cur_fil_name}\")\n",
    "                ax1.set_ylabel('Amplitude')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                # Plot 2: Grad-CAM saliency\n",
    "                ax2.plot(cam_resized, color='red', linewidth=2, label='Saliency')\n",
    "                ax2.fill_between(range(len(cam_resized)), cam_resized, alpha=0.3, color='red')\n",
    "                ax2.set_xlabel('Time steps')\n",
    "                ax2.set_ylabel('Saliency')\n",
    "                ax2.set_ylim(0, 1)\n",
    "                ax2.set_title('Grad-CAM Saliency Map (Which features the model used for prediction)')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "\n",
    "                plt.tight_layout()\n",
    "\n",
    "                plt_fil_name = f\"{cur_fund_freq}-{cur_fil_name}_{comb_lst}_crepe\"\n",
    "                plt.savefig(os.path.join(dir_path, f\"{plt_fil_name}_gradcam_crepe.png\"), dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            # csv_file.close()\n",
    "\n",
    "        # cur_res_dict[key] = cur_results\n",
    "        # pkl_path = os.path.join(dir_path, f\"{key}_results\")\n",
    "        # pickle.dump([cur_res_dict], open(f\"{pkl_path}\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing only ppsp - CORRECTED VERSION\n",
    "\"\"\"\n",
    "import warnings, pickle\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mode = \"block\"\n",
    "all_combs_lists = [[3]]\n",
    "import torch\n",
    "from models import fpn_2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "with open(f\"../conv2d_data/conv2d_psd_scaled_sfnds_1up_{mode}_test.pkl\", \"rb\") as f:\n",
    "    loaded_dict_test = pickle.load(f)\n",
    "\n",
    "for ind, comb_lst in enumerate(all_combs_lists):\n",
    "\n",
    "    dir_path = f\"./conv2d_data/pred_plots/{comb_lst}/\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Initialize PPSP model (not ppsp_1up)\n",
    "    ppsp_model = fpn_2.PPSP(in_channels=len(comb_lst), out_channels=32)\n",
    "\n",
    "    model_name = f\"../1ppsp_weights/best_model_weights_fan5_fan3_bldc_fpn2\"  # Update with your actual PPSP weights path\n",
    "    ppsp_model.load_state_dict(torch.load(f\"{model_name}\", map_location=torch.device('cpu')))\n",
    "    ppsp_model.eval()\n",
    "\n",
    "    ### CORRECTED: Choose appropriate target layer for PPSP\n",
    "    # Option 1: Late decoder layer (good for final predictions)\n",
    "    target_layer_ppsp = ppsp_model.conv_output1.c\n",
    "\n",
    "    # Option 2: Bottleneck layer (good for understanding encoder features)\n",
    "    # target_layer_ppsp = ppsp_model.conv_block10.c\n",
    "\n",
    "    # Option 3: Early decoder layer (good for understanding feature fusion)\n",
    "    # target_layer_ppsp = ppsp_model.conv_concat.c\n",
    "\n",
    "    grad_cam_ppsp = GradCAM_PPSP(ppsp_model, target_layer_ppsp)\n",
    "\n",
    "    processed_test = process_test_dict(\n",
    "        loaded_dict_test,\n",
    "        row_indices=comb_lst,\n",
    "        col_size=1024,\n",
    "        output_format=\"channels_last\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    for key, (X_test, y_test, fund_freq, distances, file_names, orig_sig) in processed_test.items():\n",
    "        cur_res_dict = {}\n",
    "        print(f\"Key: {key}\")\n",
    "\n",
    "        for sample_ind in range( len(distances)):  # Test with 3 samples\n",
    "            cur_fund_freq = fund_freq[sample_ind]\n",
    "            cur_fil_name = file_names[sample_ind]\n",
    "\n",
    "            cur_x = np.array(X_test[sample_ind])[:, :, :]\n",
    "            torch_x = torch.FloatTensor(cur_x).to('cpu')\n",
    "            cur_y = np.array(y_test[sample_ind])\n",
    "\n",
    "            torch_x.requires_grad_()\n",
    "\n",
    "            with torch.enable_grad():\n",
    "                cur_prediction = ppsp_model(torch_x)\n",
    "\n",
    "            fund_pred = torch.sigmoid(cur_prediction).squeeze(1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            cur_fund_prediction_norm = (fund_pred - np.min(fund_pred)) / (np.max(fund_pred) - np.min(fund_pred) + 1e-12)\n",
    "            binary_cur_truth = cur_y\n",
    "            bin_fund_pred = np.where(cur_fund_prediction_norm > 0.01, 1, 0)\n",
    "\n",
    "            plt_fil_name = f\"{cur_fund_freq}-{cur_fil_name}_{comb_lst}\"\n",
    "\n",
    "            ### Generate Grad-CAM for PPSP\n",
    "            # try:\n",
    "            with torch.enable_grad():\n",
    "                cam_ppsp, _ = grad_cam_ppsp(torch_x)\n",
    "\n",
    "            ### Convert CAM to same length as input\n",
    "            cam_ppsp = cam_ppsp.squeeze()\n",
    "\n",
    "            if cam_ppsp.ndim > 1:\n",
    "                cam_ppsp = cam_ppsp[0]\n",
    "\n",
    "            # Interpolate CAM to match input length (1024)\n",
    "            cam_resized = np.interp(np.linspace(0, len(cam_ppsp)-1, 1024),\n",
    "                                   np.arange(len(cam_ppsp)), cam_ppsp)\n",
    "\n",
    "        #     if distances[sample_ind] not in cur_res_dict:\n",
    "        #         cur_res_dict[distances[sample_ind]] = [[cur_fil_name, cur_fund_freq,X_test[sample_ind],y_test[sample_ind],fund_pred,cur_fund_prediction_norm,binary_cur_truth,cam_ppsp,cam_resized]]\n",
    "        #     else:\n",
    "        #         cur_res_dict[distances[sample_ind]].append([cur_fil_name, cur_fund_freq,X_test[sample_ind],y_test[sample_ind],fund_pred,cur_fund_prediction_norm,binary_cur_truth,cam_ppsp,cam_resized])\n",
    "        #\n",
    "        # pkl_path = os.path.join(dir_path, f\"{key}_ppsp_cam\")\n",
    "        # pickle.dump([cur_res_dict], open(f\"{pkl_path}\", \"wb\"))\n",
    "\n",
    "            # Create comprehensive visualization\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "            # Plot 1: Input signals\n",
    "            for ind_x in range(cur_x.squeeze(0).shape[0]):\n",
    "                ax1.plot(cur_x.squeeze(0)[ind_x], linewidth=1.2, alpha=0.7,\n",
    "                        label=f'Channel {ind_x}' if ind_x == 0 else \"\")\n",
    "\n",
    "            ax1.plot(binary_cur_truth, linewidth=2, label=\"Ground Truth\", alpha=0.9, color='black')\n",
    "            ax1.legend(loc='upper right')\n",
    "            ax1.set_title(f\"PPSP - Input Signals - {cur_fund_freq}Hz - {cur_fil_name}\")\n",
    "            ax1.set_ylabel('Amplitude')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # Plot 2: Predictions with saliency overlay\n",
    "            ax2.plot(cur_fund_prediction_norm, linewidth=2, label=\"f0 Prediction\", alpha=0.8, color='blue')\n",
    "            ax2.plot(binary_cur_truth, linewidth=1.5, label=\"Ground Truth\", alpha=0.6, color='black', linestyle='--')\n",
    "\n",
    "            # Add saliency as background color\n",
    "            x_axis = np.arange(len(cam_resized))\n",
    "            ax2.fill_between(x_axis, 0, 1, where=cam_resized > 0.7,\n",
    "                           alpha=0.3, color='red', label='High Saliency')\n",
    "            ax2.fill_between(x_axis, 0, 1, where=(cam_resized > 0.3) & (cam_resized <= 0.7),\n",
    "                           alpha=0.2, color='orange', label='Medium Saliency')\n",
    "\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylabel('Normalized Output')\n",
    "            ax2.set_ylim(0, 1)\n",
    "            ax2.set_title('Model Predictions with Saliency Overlay')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            # Plot 3: Saliency map\n",
    "            ax3.plot(cam_resized, color='red', linewidth=2, label='Saliency')\n",
    "            ax3.fill_between(range(len(cam_resized)), cam_resized, alpha=0.3, color='red')\n",
    "            ax3.set_xlabel('Time steps')\n",
    "            ax3.set_ylabel('Saliency')\n",
    "            ax3.set_ylim(0, 1)\n",
    "            ax3.set_title('Grad-CAM Saliency Map (Decoder Features Used for Prediction)')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt_fil_name = f\"{cur_fund_freq}-{cur_fil_name}_{comb_lst}_ppsp\"\n",
    "            plt.savefig(os.path.join(dir_path, f\"{plt_fil_name}_gradcam_ppsp.png\"), dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            # plt.show()\n",
    "            #\n",
    "            # # Print some diagnostic information\n",
    "            # print(f\"PPSP - CAM range: [{cam_resized.min():.3f}, {cam_resized.max():.3f}]\")\n",
    "            # print(f\"PPSP - High saliency regions: {np.sum(cam_resized > 0.7)} timepoints\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
